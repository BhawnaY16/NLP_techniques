{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## TOKENIZATION"
      ],
      "metadata": {
        "id": "rqe8hs8GgLPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEhT16IxgYwe",
        "outputId": "0fd66d99-1288-4c4e-b2d4-929f94b10d3f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=\"\"\"Hey!I am creating tokens.\n",
        "Playing with NLP libraries.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yxm5SCAlgbqI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ-6uL-nhiIS",
        "outputId": "a08594d5-5bb0-4dec-e5a5-f2db89cc1f9c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey!I am creating tokens.\n",
            "Playing with NLP libraries.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH94zm-EjW3c",
        "outputId": "895e1c8a-e538-4303-82b3-d5c11cd16c66"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting paragraph into sentence\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "documents=sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "mJKlY6nthlqE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KubKb897izaU",
        "outputId": "cba9ff50-ea4a-4956-fe08-084eaf052f6b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentences in documents:\n",
        "  print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkXVXEBCkBfe",
        "outputId": "2ba8134e-7618-4b87-fdc0-143fd0c06c6b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey!I am creating tokens.\n",
            "Playing with NLP libraries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Converting paragraph/sentence into words\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjdyVb0DkdhJ",
        "outputId": "847d6466-9415-46ea-ecaa-c033b347efd1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hey',\n",
              " '!',\n",
              " 'I',\n",
              " 'am',\n",
              " 'creating',\n",
              " 'tokens',\n",
              " '.',\n",
              " 'Playing',\n",
              " 'with',\n",
              " 'NLP',\n",
              " 'libraries',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentences in documents:\n",
        "  print(word_tokenize(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90DnYkjxkdkF",
        "outputId": "819f29b1-f43b-464a-fc72-fb975f1de7d7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hey', '!', 'I', 'am', 'creating', 'tokens', '.']\n",
            "['Playing', 'with', 'NLP', 'libraries', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MJkj440kdmw",
        "outputId": "4dbde904-26e2-4c73-dbeb-7ba882dc66ca"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hey',\n",
              " '!',\n",
              " 'I',\n",
              " 'am',\n",
              " 'creating',\n",
              " 'tokens',\n",
              " '.',\n",
              " 'Playing',\n",
              " 'with',\n",
              " 'NLP',\n",
              " 'libraries',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRp8ghyjkdpk",
        "outputId": "cd9fb554-3c8e-4950-b961-2c41dcd7481b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hey',\n",
              " '!',\n",
              " 'I',\n",
              " 'am',\n",
              " 'creating',\n",
              " 'tokens.',\n",
              " 'Playing',\n",
              " 'with',\n",
              " 'NLP',\n",
              " 'libraries',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "UZU4mis4nGKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=[\"eating\",\"eaten\",\"finalize\",\"program\",\"programming\",\"eats\",\"writing\",\"written\",\"wrote\",\"writes\"]"
      ],
      "metadata": {
        "id": "vEMTFtSzkdvV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemming= PorterStemmer()\n",
        "for word in words:\n",
        "  print(word+\"---->\"+stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn1lz60NkdyU",
        "outputId": "0a1e48c7-08ac-4738-84bf-95d80e78459d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eaten---->eaten\n",
            "finalize---->final\n",
            "program---->program\n",
            "programming---->program\n",
            "eats---->eat\n",
            "writing---->write\n",
            "written---->written\n",
            "wrote---->wrote\n",
            "writes---->write\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('sportingly')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "taYR4jzfuGvK",
        "outputId": "474cbf5f-ff31-40b3-a468-2bd8a250e390"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sportingli'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$', min=4)"
      ],
      "metadata": {
        "id": "8_hTu3VBqqcE"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('eating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3DrJWHKAqqk2",
        "outputId": "c3dbbe9c-944e-4584-8317-3d7529aff6c3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowballstemmer=SnowballStemmer('english')\n",
        "for word in words:\n",
        "  print(word+\"---->\"+snowballstemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV4kqDVJqqpD",
        "outputId": "51906034-6621-4f81-a064-ce7fecb2317f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eaten---->eaten\n",
            "finalize---->final\n",
            "program---->program\n",
            "programming---->program\n",
            "eats---->eat\n",
            "writing---->write\n",
            "written---->written\n",
            "wrote---->wrote\n",
            "writes---->write\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowballstemmer.stem('sportingly')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "59Bzv_gxqqsJ",
        "outputId": "f87ae04d-106d-4011-e109-a823cb6915d7"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sport'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LEMMATIZATION"
      ],
      "metadata": {
        "id": "6db83JIW4irI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_1Cssmj5ztk",
        "outputId": "014fad00-4cda-4c6f-c3c8-4e34ef808326"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('reading',pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4S17MB0c43j3",
        "outputId": "fa661735-1783-4374-8132-056059ca2bfe"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'read'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STmZTV9O5rTl",
        "outputId": "d6069404-2cc6-473c-b71a-372d22a252f5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eaten---->eat\n",
            "finalize---->finalize\n",
            "program---->program\n",
            "programming---->program\n",
            "eats---->eat\n",
            "writing---->write\n",
            "written---->write\n",
            "wrote---->write\n",
            "writes---->write\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('sportingly',pos='n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ASKnPrMX6rni",
        "outputId": "a9b418b5-9625-40e8-93f9-c5ef3c9dd571"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sportingly'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STOPWORDS"
      ],
      "metadata": {
        "id": "EzUCw_YV8P5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"\"\"The ecology must have energy flow in order to survive. The economy is built on the flow of money.\n",
        "Both in the forest ecosystem and economy one element is related to other elements.\n",
        "The economy as a whole suffers when one sector flounders. As the flow of money in the economy climbs, inflation rises as well. An ecosystem's main quality is resilience.\n",
        "The more resilient an ecosystem is, the further it will grow. The capacity of an ecosystem to survive in an ecological disturbance while preserving its fundamental cycles of food and energy is the resilience of the ecosystem. The more diverse the economy and forest, the more stable it will be.\n",
        "Economic resilience expands to include three key characteristics in the context of economic development: the capacity to swiftly bounce back from shocks, to absorb shocks, and to completely avoid shocks.\n",
        "A supply chain in an economy is a network of companies and individuals involved in the production and distribution of goods and services. Producers, vendors, warehouses, shipping firms, distribution hubs, and merchants are all included.\n",
        "Product creation, marketing, operations, distribution, financing, and customer support are some of its core responsibilities. In ecology, productivity refers to the speed at which an ecosystem produces biomass.\n",
        "It is often referred to as the energy that plants store during photosynthesis. By subtracting the energy lost via respiration from gross primary production, net primary productivity is calculated.\n",
        "The economy's Net Domestic Production (NDP) is derived by subtracting depreciation from its Gross Domestic Product (GDP).\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hFbMpTRa8UoR"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "n8o5SB8j8wy2"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sVV_nPU-KAC",
        "outputId": "db83d768-3daf-4cc2-8247-3125a990e204"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oQWrqDN28w1e",
        "outputId": "df7a76eb-aca8-4d04-c081-b53b11c263e3"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()\n",
        "sentences=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "70n46bTQ8w4j"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o1z8hpYDmT0",
        "outputId": "e83bcc96-d6af-405b-ccdf-82af38c6c069"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i]=' '.join(words)"
      ],
      "metadata": {
        "id": "UR0TUsBtD38_"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElaUWrFKEBGw",
        "outputId": "efefe869-530c-412c-d5d2-9c35efca986b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the ecolog must energi flow order surviv .',\n",
              " 'the economi built flow money .',\n",
              " 'both forest ecosystem economi one element relat element .',\n",
              " 'the economi whole suffer one sector flounder .',\n",
              " 'as flow money economi climb , inflat rise well .',\n",
              " \"an ecosystem 's main qualiti resili .\",\n",
              " 'the resili ecosystem , grow .',\n",
              " 'the capac ecosystem surviv ecolog disturb preserv fundament cycl food energi resili ecosystem .',\n",
              " 'the divers economi forest , stabl .',\n",
              " 'econom resili expand includ three key characterist context econom develop : capac swiftli bounc back shock , absorb shock , complet avoid shock .',\n",
              " 'a suppli chain economi network compani individu involv product distribut good servic .',\n",
              " 'produc , vendor , warehous , ship firm , distribut hub , merchant includ .',\n",
              " 'product creation , market , oper , distribut , financ , custom support core respons .',\n",
              " 'in ecolog , product refer speed ecosystem produc biomass .',\n",
              " 'it often refer energi plant store photosynthesi .',\n",
              " 'by subtract energi lost via respir gross primari product , net primari product calcul .',\n",
              " \"the economi 's net domest product ( ndp ) deriv subtract depreci gross domest product ( gdp ) .\"]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowballstemmer=SnowballStemmer('english')\n",
        "for i in range (len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i]=' '.join(words)\n",
        "\n",
        "  print(sentences[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJgdtlmWECBH",
        "outputId": "1fc34d0d-6e5b-4b00-d9b0-23ae04aabfe5"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ecolog must energi flow order surviv .\n",
            "economi built flow money .\n",
            "forest ecosystem economi one element relat element .\n",
            "economi whole suffer one sector flounder .\n",
            "flow money economi climb , inflat rise well .\n",
            "ecosystem 's main qualiti resili .\n",
            "resili ecosystem , grow .\n",
            "capac ecosystem surviv ecolog disturb preserv fundament cycl food energi resili ecosystem .\n",
            "diver economi forest , stabl .\n",
            "econom resili expand includ three key characterist context econom develop : capac swift bounc back shock , absorb shock , complet avoid shock .\n",
            "suppli chain economi network compani individu involv product distribut good servic .\n",
            "produc , vendor , wareh , ship firm , distribut hub , merchant includ .\n",
            "product creation , market , oper , distribut , financ , custom support core respon .\n",
            "ecolog , product refer speed ecosystem produc biomass .\n",
            "often refer energi plant store photosynthesi .\n",
            "subtract energi lost via respir gross primari product , net primari product calcul .\n",
            "economi 's net domest product ( ndp ) deriv subtract depreci gross domest product ( gdp ) .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "I7qEOxE2Ewxj"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[lemmatizer.lemmatize(word,pos=\"v\") for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i]=' '.join(words)\n",
        "\n",
        "  print(sentences[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lr1je4dE83a",
        "outputId": "3859c1f4-278b-448c-f81c-54e607b24792"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ecolog must energi flow order surviv .\n",
            "economi build flow money .\n",
            "forest ecosystem economi one element relat element .\n",
            "economi whole suffer one sector flounder .\n",
            "flow money economi climb , inflat rise well .\n",
            "ecosystem 's main qualiti resili .\n",
            "resili ecosystem , grow .\n",
            "capac ecosystem surviv ecolog disturb preserv fundament cycl food energi resili ecosystem .\n",
            "diver economi forest , stabl .\n",
            "econom resili expand includ three key characterist context econom develop : capac swift bounc back shock , absorb shock , complet avoid shock .\n",
            "suppli chain economi network compani individu involv product distribut good servic .\n",
            "produc , vendor , wareh , ship firm , distribut hub , merchant includ .\n",
            "product creation , market , oper , distribut , financ , custom support core respon .\n",
            "ecolog , product refer speed ecosystem produc biomass .\n",
            "often refer energi plant store photosynthesi .\n",
            "subtract energi lose via respir gross primari product , net primari product calcul .\n",
            "economi 's net domest product ( ndp ) deriv subtract depreci gross domest product ( gdp ) .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tySef7vTLhlF",
        "outputId": "fbed0e77-90df-43f9-a518-b0299342bd76"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[word for word in words if word not in set(stopwords.words('english'))]\n",
        "  pos_tag=nltk.pos_tag(words)\n",
        "\n",
        "  print(pos_tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmn_AzlTLo5P",
        "outputId": "9adbf121-cbf8-4c11-fcc3-7d631b690383"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ecolog', 'NN'), ('must', 'MD'), ('energi', 'VB'), ('flow', 'JJ'), ('order', 'NN'), ('surviv', 'NN'), ('.', '.')]\n",
            "[('economi', 'NN'), ('build', 'VB'), ('flow', 'JJ'), ('money', 'NN'), ('.', '.')]\n",
            "[('forest', 'JJS'), ('ecosystem', 'NN'), ('economi', 'VBP'), ('one', 'CD'), ('element', 'NN'), ('relat', 'JJ'), ('element', 'NN'), ('.', '.')]\n",
            "[('economi', 'JJ'), ('whole', 'JJ'), ('suffer', 'NN'), ('one', 'CD'), ('sector', 'NN'), ('flounder', 'NN'), ('.', '.')]\n",
            "[('flow', 'JJ'), ('money', 'NN'), ('economi', 'NN'), ('climb', 'NN'), (',', ','), ('inflat', 'JJ'), ('rise', 'NN'), ('well', 'RB'), ('.', '.')]\n",
            "[('ecosystem', 'NN'), (\"'s\", 'POS'), ('main', 'JJ'), ('qualiti', 'NN'), ('resili', 'NN'), ('.', '.')]\n",
            "[('resili', 'NN'), ('ecosystem', 'NN'), (',', ','), ('grow', 'NN'), ('.', '.')]\n",
            "[('capac', 'NN'), ('ecosystem', 'NN'), ('surviv', 'JJ'), ('ecolog', 'NN'), ('disturb', 'NN'), ('preserv', 'NN'), ('fundament', 'NN'), ('cycl', 'NN'), ('food', 'NN'), ('energi', 'NN'), ('resili', 'NN'), ('ecosystem', 'NN'), ('.', '.')]\n",
            "[('diver', 'RB'), ('economi', 'JJ'), ('forest', 'JJS'), (',', ','), ('stabl', 'NN'), ('.', '.')]\n",
            "[('econom', 'NN'), ('resili', 'NN'), ('expand', 'NN'), ('includ', 'NN'), ('three', 'CD'), ('key', 'JJ'), ('characterist', 'NN'), ('context', 'NN'), ('econom', 'NN'), ('develop', 'NN'), (':', ':'), ('capac', 'NN'), ('swift', 'NN'), ('bounc', 'NN'), ('back', 'RB'), ('shock', 'NN'), (',', ','), ('absorb', 'NN'), ('shock', 'NN'), (',', ','), ('complet', 'NN'), ('avoid', 'NN'), ('shock', 'NN'), ('.', '.')]\n",
            "[('suppli', 'NN'), ('chain', 'NN'), ('economi', 'JJ'), ('network', 'NN'), ('compani', 'NN'), ('individu', 'NN'), ('involv', 'JJ'), ('product', 'NN'), ('distribut', 'NN'), ('good', 'JJ'), ('servic', 'NN'), ('.', '.')]\n",
            "[('produc', 'NN'), (',', ','), ('vendor', 'NN'), (',', ','), ('wareh', 'NN'), (',', ','), ('ship', 'JJ'), ('firm', 'NN'), (',', ','), ('distribut', 'NN'), ('hub', 'NN'), (',', ','), ('merchant', 'NN'), ('includ', 'NN'), ('.', '.')]\n",
            "[('product', 'NN'), ('creation', 'NN'), (',', ','), ('market', 'NN'), (',', ','), ('oper', 'JJR'), (',', ','), ('distribut', 'NN'), (',', ','), ('financ', 'NN'), (',', ','), ('custom', 'JJ'), ('support', 'NN'), ('core', 'NN'), ('respon', 'NN'), ('.', '.')]\n",
            "[('ecolog', 'NN'), (',', ','), ('product', 'NN'), ('refer', 'NN'), ('speed', 'NN'), ('ecosystem', 'NN'), ('produc', 'JJ'), ('biomass', 'NN'), ('.', '.')]\n",
            "[('often', 'RB'), ('refer', 'VBP'), ('energi', 'JJ'), ('plant', 'NN'), ('store', 'NN'), ('photosynthesi', 'NN'), ('.', '.')]\n",
            "[('subtract', 'JJ'), ('energi', 'CC'), ('lose', 'JJ'), ('via', 'IN'), ('respir', 'NN'), ('gross', 'JJ'), ('primari', 'NN'), ('product', 'NN'), (',', ','), ('net', 'JJ'), ('primari', 'NN'), ('product', 'NN'), ('calcul', 'NN'), ('.', '.')]\n",
            "[('economi', 'NN'), (\"'s\", 'POS'), ('net', 'JJ'), ('domest', 'NN'), ('product', 'NN'), ('(', '('), ('ndp', 'JJ'), (')', ')'), ('deriv', 'NN'), ('subtract', 'JJ'), ('depreci', 'JJ'), ('gross', 'JJ'), ('domest', 'JJS'), ('product', 'NN'), ('(', '('), ('gdp', 'JJ'), (')', ')'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Name-Entity Recognition"
      ],
      "metadata": {
        "id": "w_Wxv2OaNkt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"Barack Obama gave a speech at the United Nations headquarters in New York on 22-02-2022.\""
      ],
      "metadata": {
        "id": "LFm5wiMOMXXK"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=nltk.word_tokenize(sentence)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNJoS385MXau",
        "outputId": "151ebd19-2de0-4e9e-964f-1a1d1971032f"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Barack', 'Obama', 'gave', 'a', 'speech', 'at', 'the', 'United', 'Nations', 'headquarters', 'in', 'New', 'York', 'on', '22-02-2022', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag_elements=nltk.pos_tag(words)"
      ],
      "metadata": {
        "id": "c4Zhn-85OfCC"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.ne_chunk(tag_elements)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "pxSDTocAOoxC",
        "outputId": "5e9e2169-b5a5-4e51-d607-3d269fbd6a2e"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('PERSON', [('Barack', 'NNP')]), Tree('PERSON', [('Obama', 'NNP')]), ('gave', 'VBD'), ('a', 'DT'), ('speech', 'NN'), ('at', 'IN'), ('the', 'DT'), Tree('ORGANIZATION', [('United', 'NNP'), ('Nations', 'NNPS')]), ('headquarters', 'NNS'), ('in', 'IN'), Tree('GPE', [('New', 'NNP'), ('York', 'NNP')]), ('on', 'IN'), ('22-02-2022', 'JJ'), ('.', '.')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,864.0,168.0\" width=\"864px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"7.40741%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Barack</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.7037%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.40741%\" x=\"7.40741%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Obama</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.1111%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.55556%\" x=\"14.8148%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">gave</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.5926%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.7037%\" x=\"20.3704%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.2222%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.40741%\" x=\"24.0741%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">speech</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"27.7778%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.7037%\" x=\"31.4815%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">at</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"33.3333%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.62963%\" x=\"35.1852%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.5%\" y1=\"20px\" y2=\"48px\" /><svg width=\"15.7407%\" x=\"39.8148%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"47.0588%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">United</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.5294%\" y1=\"20px\" y2=\"48px\" /><svg width=\"52.9412%\" x=\"47.0588%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Nations</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNPS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.5294%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.6852%\" y1=\"20px\" y2=\"48px\" /><svg width=\"12.963%\" x=\"55.5556%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">headquarters</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.037%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.7037%\" x=\"68.5185%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.3704%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.1852%\" x=\"72.2222%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"45.4545%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">New</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.7273%\" y1=\"20px\" y2=\"48px\" /><svg width=\"54.5455%\" x=\"45.4545%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">York</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.7273%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.3148%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.7037%\" x=\"82.4074%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">on</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84.2593%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.1111%\" x=\"86.1111%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">22-02-2022</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"91.6667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.77778%\" x=\"97.2222%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.6111%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0wlFik-bPQqB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}